{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def step(x):\n",
    "    return np.heaviside(x, 1)\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y = step(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Step Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()\n",
    "\n",
    "##################################\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = tanh(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Hyperbolic Tangent (tanh) Function')\n",
    "plt.show()\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Rectified Linear Unit (ReLU) Function')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha*x, x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = leaky_relu(x, alpha=0.01)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Leaky ReLU Activation Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Create input values\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "elu1 = elu(x, alpha=1.0)\n",
    "\n",
    "plt.plot(x, elu1, label='alpha = 1.0')\n",
    "\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ELU Activation Function')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "########################################\n",
    "\n",
    "\n",
    "def selu(x, alpha=1.67326, scale=1.0507):\n",
    "    positive = scale * (np.maximum(x, 0))\n",
    "    negative = scale * (alpha * (np.exp(np.minimum(x, 0)) - 1))\n",
    "    return positive + negative\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y = selu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"SELU Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0de16cae6eac96ebe3f624ce1fe57954a171ba008c407f7ddd3d004476a18b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
